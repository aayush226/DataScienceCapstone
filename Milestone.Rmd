# **Milestone Report: Capstone Project**

**Aayush Shah**

## **Executive Summary**

This is the Milestone Report for the Coursera Data Science Capstone project. This report was generated to perform exploratory data analysis for making a predictive algorithm using data provided by SwiftKey. Further plan to create a Shiny App is also discussed.

**Loading Libraries**

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(stringi)
library(LaF)
library(tm)
library(ggplot2)
library(tidytext)
library(tidyr)
library(wordcloud)
```

**Downloading Data**

```{r echo=TRUE, cache=TRUE}
filename <- "Coursera-SwiftKey.zip"
if (!file.exists(filename)){
  url<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
  download.file(url, filename, method="curl")
}  
if (!file.exists("final")) { 
  unzip(filename) 
}

```

**Reading the Data**

Although data is available for many languages, we concern ourselves with data in English language only.

```{r warning=FALSE, cache=TRUE}
blogs <- readLines(paste0(getwd(),"/final/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
news<- readLines(paste0(getwd(),"/final/en_US/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
twitter<- readLines(paste0(getwd(),"/final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
```

**Summarizing the Data**

```{r cache=TRUE}
lblogs<-length(blogs)
lnews<-length(news)
ltwitter<-length(twitter)
tibble(File = c("blogs", "news", "twitter"),Lines = c(lblogs, lnews, ltwitter),Words = c(sum(stri_count_words(blogs)), sum(stri_count_words(news)), sum(stri_count_words(twitter))),Characters = c(sum(nchar(blogs)),sum(nchar(news)),sum(nchar(twitter))))
```

**Sampling the Data**

```{r cache=TRUE}
set.seed(123)
sample1 <- sample(blogs, size = 0.01*lblogs, replace = TRUE)
sample2 <- sample(news, size = 0.01*lnews, replace = TRUE)
sample3 <- sample(twitter, size = 0.01*ltwitter, replace = TRUE)
sampledata <- c(sample1, sample2, sample3)
fileConn<-file("sample.txt")
writeLines(sampledata, fileConn)
close(fileConn)
```

**Constructing and Cleaning the Corpus**

Removing stopwords, punctuation, whitespaces, numbers, URL's and profanities from the corpus.
(List of Profane Words is found [here](https://github.com/aiyuswa/DataScienceCapstone/blob/master/profane_words.txt))


```{r warning=FALSE, cache=TRUE}
#getTransformations()
conn <- file("sample.txt")
corpus <- readLines(conn)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, content_transformer(function(x) iconv(x, to="UTF-8", sub="byte")))
corpus <- tm_map(corpus, content_transformer(tolower)) 
corpus <- tm_map(corpus, content_transformer(removePunctuation), preserve_intra_word_dashes=TRUE)
corpus <- tm_map(corpus, content_transformer(removeNumbers))
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
corpus <- tm_map(corpus, removeWords, stopwords("english")) 
profanityWords = readLines('profane_words.txt')
corpus <- tm_map(corpus,removeWords, profanityWords)
corpus <- tm_map(corpus, stripWhitespace) 
saveRDS(corpus, file = "corpus.RData")
corpus <- readRDS("corpus.RData")
```

**Converting Corpus back to Data Frame**

```{r}
corpus <- data.frame(text = get("content", corpus), stringsAsFactors = FALSE)
```

**Tokenizing the Data Frame**

Converting the data frame into unigrams, bigrams and trigrams.

```{r cache=TRUE}
text_df<-tibble(line=1:length(corpus),corpus)
text_df1g<-text_df%>%
  unnest_tokens(word,text)
text_df2g<-text_df%>%
  unnest_tokens(words,text,token="ngrams",n=2)
text_df3g<-text_df%>%
  unnest_tokens(words,text,token="ngrams",n=3)
```

**Visualizing Unigrams**

```{r cache=TRUE}
count1g<-text_df1g%>%
  count(word, sort = TRUE) 
text_df1g %>%
  count(word, sort = TRUE) %>%
  filter(n > 1100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

**Visualizing Bigrams**

```{r cache=TRUE}
count2g<-text_df2g%>%
  count(words, sort = TRUE) 
text_df2g %>%
  count(words, sort = TRUE) %>%
  filter(n > 75) %>%
  mutate(words = reorder(words, n)) %>%
  ggplot(aes(words, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

wordcloud(words = count2g$words, freq = count2g$n,scale=c(2,.5), min.freq = 50,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

**Visualizing Trigrams**

```{r warning=FALSE, cache=TRUE}
count3g<-text_df3g%>%
  count(words, sort = TRUE) 
text_df3g %>%
  count(words, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(words = reorder(words, n)) %>%
  ggplot(aes(words, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

wordcloud(words = count3g$words, freq = count3g$n,scale=c(2,.5), min.freq = 5,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

## **Plan of Action**

So after completing the exploratory data analysis, the next step is to finalize the prediction model and deploy the algorithm with a shiny app which will predict the most probable word to follow an input from the user.

